{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CatDCGAN64_48",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKlGAKYFB-o1"
      },
      "source": [
        "'''This code cell is for preprocessing image data from google drive zip folder'''\n",
        "!gdown https://drive.google.com/uc?id=1Lbk_pwATorDcUyk9VcGYzomLR4Rhj-lv #Downloading zip file from google drive\n",
        "zip_path = '/content/Cat_GAN_1.zip' #Getting the path \n",
        "!unzip -q Cat_GAN_1.zip #Unzipping the folder\n",
        "!rm Cat_GAN_1.zip #Removing the zip folder\n",
        "import os #Imported required libraries for this step\n",
        "import pdb\n",
        "import cv2\n",
        "import operator\n",
        "from PIL import Image\n",
        "os.mkdir(\"Training\") #making the new Training and testing folders\n",
        "dirs = [\"Domino\", \"Stormy\"] #Making the names for the subdirec\n",
        "pardirs = [\"/content/Training\"]\n",
        "for pardir in pardirs:\n",
        "  for dir in dirs: \n",
        "    path = os.path.join(pardir, dir)\n",
        "    os.mkdir(path)\n",
        "datadir1 = \"/content/Cat_Classifier_Images_70-30 /Train/Domino\"\n",
        "datadir2 = \"/content/Cat_Classifier_Images_70-30 /Train/Stormy\"\n",
        "datadir3 = \"/content/Cat_Classifier_Images_70-30 /Test/Domino\"\n",
        "datadir4 = \"/content/Cat_Classifier_Images_70-30 /Test/Stormy\"\n",
        "filelist1 = sorted(os.listdir(datadir1), key = lambda fname: int(fname.split(\"_\")[0][-4:]))\n",
        "filelist2 = sorted(os.listdir(datadir2), key = lambda fname: int(fname.split(\"_\")[0][-4:]))\n",
        "filelist3 = sorted(os.listdir(datadir3), key = lambda fname: int(fname.split(\"_\")[0][-4:]))\n",
        "filelist4 = sorted(os.listdir(datadir4), key = lambda fname: int(fname.split(\"_\")[0][-4:]))\n",
        "datadirs = [filelist1, filelist2, filelist3, filelist4]\n",
        "inc = 0\n",
        "idom = 0\n",
        "istorm = 0\n",
        "for filelist in datadirs:\n",
        "  for fil in filelist:\n",
        "    if inc == 0:\n",
        "      path = \"/content/Cat_Classifier_Images_70-30 /Train/Domino/\" + fil\n",
        "      idom += 1\n",
        "      img = cv2.imread(path)\n",
        "      imgResized = cv2.resize(img, (48, 64))\n",
        "      cv2.imwrite('/content/Training/Domino/DominoTR%03i.jpg' %idom, imgResized)\n",
        "    elif inc == 1:\n",
        "      path = \"/content/Cat_Classifier_Images_70-30 /Train/Stormy/\" + fil\n",
        "      istorm += 1\n",
        "      img = cv2.imread(path)\n",
        "      imgResized = cv2.resize(img, (48, 64))\n",
        "      cv2.imwrite('/content/Training/Stormy/StormyTR%03i.jpg' %istorm, imgResized)\n",
        "    elif inc == 2:\n",
        "      path = \"/content/Cat_Classifier_Images_70-30 /Test/Domino/\" + fil\n",
        "      img = cv2.imread(path)\n",
        "      idom += 1\n",
        "      imgResized = cv2.resize(img, (48, 64))\n",
        "      cv2.imwrite('/content/Training/Domino/DominoTR%03i.jpg' %idom, imgResized)\n",
        "    else:\n",
        "      path = \"/content/Cat_Classifier_Images_70-30 /Test/Stormy/\" + fil\n",
        "      img = cv2.imread(path)\n",
        "      istorm += 1\n",
        "      imgResized = cv2.resize(img, (48, 64))\n",
        "      cv2.imwrite('/content/Training/Stormy/StormyTR%03i.jpg' %istorm, imgResized)\n",
        "  inc += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8-ozHTLNV8J"
      },
      "source": [
        "import pdb\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "datadirDom = '/content/Training/Domino'\n",
        "datadirStorm = '/content/Training/Stormy'\n",
        "datadirDom = os.listdir(datadirDom)\n",
        "datadirStorm = os.listdir(datadirStorm)\n",
        "labels = []\n",
        "cat_images = []\n",
        "for fil in datadirDom:\n",
        "  path = '/content/Training/Domino/' + fil\n",
        "  image = Image.open(path)\n",
        "  horz_image = np.array(image.transpose(method = Image.FLIP_LEFT_RIGHT))\n",
        "  vert_image = np.array(image.transpose(method = Image.FLIP_TOP_BOTTOM))\n",
        "  rot_image = np.array(image.rotate(180))\n",
        "  image = np.array(image)\n",
        "  cat_images.append(image)\n",
        "  plt.imshow(image)\n",
        "  cat_images.append(horz_image)\n",
        "  cat_images.append(vert_image)\n",
        "  cat_images.append(rot_image)\n",
        "for fil in datadirStorm:\n",
        "  path = '/content/Training/Stormy/' + fil\n",
        "  image = Image.open(path)\n",
        "  horz_image = np.array(image.transpose(method = Image.FLIP_LEFT_RIGHT))\n",
        "  vert_image = np.array(image.transpose(method = Image.FLIP_TOP_BOTTOM))\n",
        "  rot_image = np.array(image.rotate(180))\n",
        "  image = np.array(image)\n",
        "  cat_images.append(image)\n",
        "  cat_images.append(horz_image)\n",
        "  cat_images.append(vert_image)\n",
        "  cat_images.append(rot_image)\n",
        "cat_images = np.array(cat_images)\n",
        "print (cat_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDJpit46Nco_"
      },
      "source": [
        "from tensorflow.keras.layers import Activation, Dense, Input\n",
        "from tensorflow.keras.layers import Conv2D, Flatten\n",
        "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import argparse\n",
        "import pdb\n",
        "def build_generator(inputs, image_size):\n",
        "    image_resize = (image_size[0] // 4, image_size[1] // 4)\n",
        "    kernel_size = 5\n",
        "    layer_filters = [128, 64, 32, 3]\n",
        "    x = inputs\n",
        "    x = Dense(image_resize[0] * image_resize[1] * 3 * layer_filters[0])(x)\n",
        "    x = Reshape((image_resize[0], image_resize[1], 3 * layer_filters[0]))(x)\n",
        "    for filters in layer_filters:\n",
        "        if filters > layer_filters[-2]:\n",
        "            strides = 2\n",
        "        else:\n",
        "            strides = 1\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    x = Reshape((64, 48, 3, 1))(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    generator = Model(inputs, x, name='generator')\n",
        "    return generator\n",
        "def build_discriminator(inputs, image_size):\n",
        "    kernel_size = 5\n",
        "    layer_filters = [32, 64, 128, 256]\n",
        "    x = inputs\n",
        "    for filters in layer_filters:\n",
        "        if filters == layer_filters[-1]:\n",
        "            strides = 1\n",
        "        else:\n",
        "            strides = 2\n",
        "        x = LeakyReLU(alpha=0.2)(x)\n",
        "        x = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same')(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(1)(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    discriminator = Model(inputs, x, name='discriminator')\n",
        "    return discriminator\n",
        "def train(models, data, params):\n",
        "    losss = []\n",
        "    accc = []\n",
        "    generator, discriminator, adversarial = models\n",
        "    x_train = data\n",
        "    batch_size, latent_size, train_steps, model_name = params\n",
        "    save_interval = 500\n",
        "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
        "    train_size = x_train.shape[0]\n",
        "    accavg = 0\n",
        "    accavg1 = 0\n",
        "    epochs = [i for i in range(train_steps)]\n",
        "    for i in range(train_steps):\n",
        "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
        "        real_images = x_train[rand_indexes]\n",
        "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "        fake_images = generator.predict(noise)\n",
        "        x = np.concatenate((real_images, fake_images))\n",
        "        y = np.ones([2 * batch_size, 1])\n",
        "        y[batch_size:, :] = 0.0\n",
        "        loss, acc = discriminator.train_on_batch(x, y)\n",
        "        losss.append(loss)\n",
        "        accc.append(acc)\n",
        "        accavg += acc\n",
        "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
        "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "        y = np.ones([batch_size, 1])\n",
        "        loss, acc = adversarial.train_on_batch(noise, y)\n",
        "        accavg1 += acc\n",
        "        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n",
        "#        print(log)\n",
        "        if (i + 1) % save_interval == 0:\n",
        "           accavg = accavg / save_interval\n",
        "           print (\"Average discriminator accuracy: \" + str(accavg))\n",
        "           accavg1 = accavg1 / save_interval\n",
        "           print (\"Average adversarial accuracy: \" + str(accavg1))\n",
        "           accavg = 0\n",
        "           accavg1 = 0\n",
        "           plot_images(generator, noise_input, show = False, step = i + 1)\n",
        "           images = generator.predict(noise_input)\n",
        "    plt.plot(epochs, losss)\n",
        "    plt.title(\"Discriminator loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "    plt.savefig('DCGAN_discriminator_loss.jpg')\n",
        "    plt.plot(epochs, accc)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Discriminator Accuracy\")\n",
        "    plt.show()\n",
        "    plt.savefig('DCGAN_discriminator?accuracy.jpg')\n",
        "    generator.save(model_name + \".h5\")\n",
        "def plot_images(generator, noise_input, show=False, step=0, model_name=\"gan\"):\n",
        "    os.makedirs(model_name, exist_ok=True)\n",
        "    filename = os.path.join(model_name, \"%05d.png\" % step)\n",
        "    images = generator.predict(noise_input)\n",
        "    plt.figure(figsize=(11.1, 11.1))\n",
        "    num_images = images.shape[0]\n",
        "    image_size = images.shape[1]\n",
        "    rows = int(math.sqrt(noise_input.shape[0]))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(rows, rows, i + 1)\n",
        "        plt.imshow(np.array(images[i - 1]).reshape((64, 48, 3)))\n",
        "        plt.axis('off')\n",
        "    plt.savefig(filename)\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close('all')\n",
        "def build_and_train_models():\n",
        "    x_train = cat_images\n",
        "    x_train = np.reshape(x_train, [x_train.shape[0], x_train.shape[1], x_train.shape[2], x_train.shape[3], 1])\n",
        "    x_train = x_train.astype('float32') / 255\n",
        "    model_name = \"cgan_cat\"\n",
        "    latent_size = 2\n",
        "    batch_size = 64\n",
        "    train_steps = 40000\n",
        "    lr = 2e-4\n",
        "    decay = 6e-8\n",
        "    input_shape = (64, 48, 3, 1)\n",
        "    label_shape = (2,)\n",
        "    image_size = (64, 48, 3)\n",
        "    inputs = Input(shape=input_shape, name='discriminator_input')\n",
        "    discriminator = build_discriminator(inputs, image_size)\n",
        "    optimizer = RMSprop(lr=lr, decay=decay)\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    discriminator.summary()\n",
        "    input_shape = (latent_size, )\n",
        "    inputs = Input(shape=input_shape, name='z_input')\n",
        "    generator = build_generator(inputs, image_size)\n",
        "    generator.summary()\n",
        "    optimizer = RMSprop(lr=lr*0.5, decay=decay*0.5)\n",
        "    discriminator.trainable = False\n",
        "    outputs = discriminator(generator(inputs))\n",
        "    adversarial = Model(inputs, outputs, name=model_name)\n",
        "    adversarial.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    adversarial.summary()\n",
        "    models = (generator, discriminator, adversarial)\n",
        "    data = x_train\n",
        "    params = (batch_size, latent_size, train_steps, model_name)\n",
        "    train(models, data, params)\n",
        "build_and_train_models()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}